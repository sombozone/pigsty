# tuned configuration
#==============================================================#
# File      :   tuned.conf (tiny)
# Desc      :   Tune operating system for tiny virtual machines
# Time      :   {{ '%Y-%m-%d %H:%M' |strftime }}
# Host      :   {{ nodename }} @ {{ inventory_hostname }}
# Path      :   /etc/tuned/tiny/tuned.conf
# Note      :   ANSIBLE MANAGED, CHANGE WITH CAUTIONS!
# License   :   AGPLv3 @ https://doc.pgsty.com/about/license
# Copyright :   2018-2025  Ruohang Feng / Vonng (rh@vonng.com)
#==============================================================#

[main]
summary=Optimize for TINY System
# include=virtual-guest

[vm]
# disable transparent hugepages
transparent_hugepages=never

[sysctl]
#-------------------------------------------------------------#
#                           KERNEL                            #
#-------------------------------------------------------------#
# disable numa balancing
kernel.numa_balancing=0

# do not schedule postgres process in group
kernel.sched_autogroup_enabled = 0

# shmall in pages
kernel.shmall = {{ (node_pages|int * 0.75)|int }}

# shmmax size in bytes
kernel.shmmax = {{ (node_mem_bytes|int * 0.75)|int }}

# total shmem segs
kernel.shmmni=8192

#-------------------------------------------------------------#
#                           Memory                            #
#-------------------------------------------------------------#
# try not using swap
vm.swappiness=10

# disable when most mem are for file cache
vm.zone_reclaim_mode=0

{% if node_overcommit_ratio|int > 0 %}
# overcommit threshhold = {{ node_overcommit_ratio }}%
vm.overcommit_memory=2
vm.overcommit_ratio={{ node_overcommit_ratio }}
{% else %}
vm.overcommit_memory=0
vm.overcommit_ratio=200
{% endif %}

# Use 5% for TINY system (10% -> 5%)
vm.dirty_background_ratio=5

# Use default 40% for TINY system (40% -> 30%)
vm.dirty_ratio=30

# Use default 3s for TINY system
#vm.dirty_expire_centisecs = 3000

# Use default 5s for TINY system
#vm.dirty_writeback_centisecs = 500

# deny access on 0x00000 - 0x10000
vm.mmap_min_addr=65536

#-------------------------------------------------------------#
#                        Filesystem                           #
#-------------------------------------------------------------#
# max open files: 382589 -> 2M
fs.file-max=2097152

# max concurrent unfinished async io, should be larger than 1M.  65536->1M
fs.aio-max-nr=1048576

#-------------------------------------------------------------#
#                          Network                            #
#-------------------------------------------------------------#
# max connection in listen queue (triggers re-trans if full)
net.core.somaxconn=8192
# for TINY system, use 2K backlog
net.core.netdev_max_backlog=2048
# tcp receive/transmit buffer default = 256KiB
net.core.rmem_default=262144
net.core.wmem_default=262144
# receive/transmit buffer limit = 4MiB
net.core.rmem_max=4194304
net.core.wmem_max=4194304

net.ipv4.tcp_rmem="4096 87380 16777216"
net.ipv4.tcp_wmem="4096 16384 16777216"
net.ipv4.udp_mem="3145728 4194304 16777216"

# features
net.ipv4.ip_forward=1
net.ipv4.ip_nonlocal_bind=1
net.ipv4.tcp_timestamps=1
net.ipv4.tcp_syncookies=0
net.ipv4.ip_local_port_range=10000 65000
# reuse timewait socket for TINY template
net.ipv4.tcp_tw_reuse=1

# syn retry
net.ipv4.tcp_synack_retries=3
net.ipv4.tcp_syn_retries=3

# keepalive: 60/20/3 for OLTP system
net.ipv4.tcp_keepalive_time=60
net.ipv4.tcp_keepalive_intvl=20
net.ipv4.tcp_keepalive_probes=3

# reduce fin timeout from 60s to 30s for TINY system
net.ipv4.tcp_fin_timeout=30
net.ipv4.tcp_max_tw_buckets=262144
# 128 -> 8192 to increase the listen queue size
net.ipv4.tcp_max_syn_backlog=4096

# 64K entries
net.netfilter.nf_conntrack_max=65536

# kubernetes only
#net.ipv4.neigh.default.gc_thresh1=80000
#net.ipv4.neigh.default.gc_thresh2=90000
#net.ipv4.neigh.default.gc_thresh3=100000
#net.bridge.bridge-nf-call-iptables=1
#net.bridge.bridge-nf-call-ip6tables=1
#net.bridge.bridge-nf-call-arptables=1


#-------------------------------------------------------------#
#                            Disk                             #
#-------------------------------------------------------------#
[disk-scheduler-vd]
type=disk
devices=vd[a-z]
elevator=none

[disk-scheduler-xvd]
type=disk
devices=xvd[a-z]
elevator=none

[disk-scheduler-nvme]
type=disk
devices=nvme*
elevator=none

